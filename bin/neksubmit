#!/bin/bash

# Define local directory
LOCAL_DIR=$(pwd)

# Get the name of the current directory
DIR_NAME=$(basename ${LOCAL_DIR})

# Define remote directory and host
REMOTE_DIR=/gpfswork/rech/vpo/rvpo014/${DIR_NAME}
REMOTE_HOST=rvpo014@jean-zay.idris.fr

# Sync files to supercomputer
rsync -avz ${LOCAL_DIR}/ ${REMOTE_HOST}:${REMOTE_DIR}/

JOB_NAME=$(date +%Y%m%d_%H%M%S)

# Create job script and update 'SBATCH -J' with JOB_NAME variable
read -r -d '' JOB_SCRIPT << EOM
#!/bin/bash 
#SBATCH --qos qos_cpu-dev
#SBATCH -J s220
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40
#SBATCH --hint=nomultithread
#SBATCH --exclusive
#SBATCH -A vpo@cpu
#SBATCH --time=02:00:00
#SBATCH --output=logfile
#SBATCH --error=logerror

cd ${SLURM_SUBMIT_DIR}
module purge
module load intel-all/2020.4
set -x
ulimit -s unlimited
ulimit -c unlimited
# export CASE='sphere'
# echo $CASE > SESSION.NAME
# echo $SLURM_SUBMIT_DIR'/' >>  SESSION.NAME
srun -n $SLURM_NTASKS ./nek5000
EOM

# Write job script to file
echo "${JOB_SCRIPT}" > jz.pbs

# Copy job script to supercomputer
scp jz.pbs ${REMOTE_HOST}:${REMOTE_DIR}/

# Submit job
ssh ${REMOTE_HOST} "cd ${REMOTE_DIR} && sbatch jz.pbs"

# Create a local file to easy acces the remote log file

# Create the local file in the same directory as the script
touch "$(dirname "$0")/$remote_logfile"

# Get the absolute path of the local file
local_path="$(realpath "$(dirname "$0")/$local_file")"

# Set up the remote tail -f functionality
tail -f "$local_path" | ssh "$REMOTE_HOST" "tail -f $REMOTE_DIR/logfile"